{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"features_extraction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNGHG9zBjGJVYz8DLm1zlRO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Global Variables definition:"],"metadata":{"id":"2IGoY7Jgg7mP"}},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"eZmOnISNxR9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PLEASE MAKE SURE TO EDIT THE PATHS TO THE DATASETS AND THE PARAMETERS SUCH AS THE CNN BACKBONE 'base' ACCORDING TO YOUR CASE SCENARIO"],"metadata":{"id":"We6KfkOPOGnr"}},{"cell_type":"code","source":["params = {'base':'resnet',\n","        'dim':(224,224),\n","        'db_path':'../Databases/LIVE VIDEO QC/Video/',\n","        'num_frames':10,\n","        'num_patches':1\n","        }"],"metadata":{"id":"ZfRCU8gvQxjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_IDs_path='../Databases/LIVE VIDEO QC/IDs_train.pickle'\n","pickle_in = open(list_IDs_path,'rb')\n","ids= pickle.load(pickle_in)\n","pickle_in.close()\n","list_IDs_path='../Databases/LIVE VIDEO QC/IDs_test.pickle'\n","pickle_in = open(list_IDs_path,'rb')\n","ids=ids+ pickle.load(pickle_in)\n","pickle_in.close()"],"metadata":{"id":"XnanGxMVQ3FH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out='../Features_UGC/resnet50/live'"],"metadata":{"id":"y6vQV_omhCJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#CNN backbone "],"metadata":{"id":"asEyDIuziMLf"}},{"cell_type":"code","source":["from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.vgg19 import VGG19\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications import DenseNet121\n","from tensorflow.keras.applications import InceptionV3"],"metadata":{"id":"RsyhJj1aiXwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Base_Model(base,weights='imagenet', include_top=False, input_shape=(299, 299, 3)):\n","    if(base=='resnet50'):\n","        return ResNet50(weights=weights, include_top=include_top, input_shape=input_shape)\n","    if(base=='vgg16'):\n","        return VGG16(weights=weights, include_top=include_top, input_shape=input_shape)\n","    if(base=='vgg19'):\n","        return VGG19(weights=weights, include_top=include_top, input_shape=input_shape)\n","    if(base=='densenet121'):\n","        return DenseNet121(weights=weights, include_top=include_top, input_shape=input_shape)\n","    if(base=='inceptionv3'):\n","        return InceptionV3(weights=weights, include_top=include_top, input_shape=input_shape)"],"metadata":{"id":"g_LI6undhDHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data generator"],"metadata":{"id":"eJKirED9iavd"}},{"cell_type":"code","source":["!pip install slidingwindow"],"metadata":{"id":"7L9lMAGnlDky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import pickle\n","import slidingwindow as sw\n","from tensorflow import keras\n","import numpy as np\n","import copy"],"metadata":{"id":"Ax5wqLnvjT-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataGenerator(keras.utils.Sequence):\n","    def __init__(self,dim=(224,224),n_channels=3,n_output=1,base='resnet',\n","                db_path='KoNViD_1k_videos/',threshold=0.5,\n","                ids=[],num_frames=8,num_patches=6):\n","        'Initialization'\n","        self.num_patches=num_patches\n","        self.num_frames=num_frames\n","        self.batch_size= 1\n","        self.dim = dim\n","        self.n_channels = n_channels\n","        self.n_output = n_output\n","        self.base=base\n","        self.db_path=db_path\n","        self.ids_path=ids\n","        self.list_IDs_temp=[]\n","        self.list_IDs=ids\n","        self.threshold=threshold\n","                \n","\n","        vidcap = cv2.VideoCapture(os.path.join(self.db_path,id))\n","        success,image = vidcap.read()\n","        ov = 0\n","        windows = sw.generate(image, sw.DimOrder.HeightWidthChannel,self.dim[0],ov)\n","            \n","        while len(windows) < self.num_patches:\n","            ov =ov+ 0.1\n","            windows = sw.generate(image, sw.DimOrder.HeightWidthChannel,self.dim[0],ov)\n","            if ov > self.threshold:\n","              break;\n","        self.ov=ov\n","\n","        global p \n","        p= len(windows)\n","\n","\n","\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs)/ self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        # Find list of IDs\n","        self.list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","        # Generate data\n","        X = self.__data_generation(self.list_IDs_temp)\n","        return X\n","    \n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        \n","\n","    def __data_generation(self, list_IDs_temp):\n","      for i, ID in enumerate(list_IDs_temp):\n","\n","        n=self.dim[0]\n","    \n","        images=[]\n","        vidcap = cv2.VideoCapture(self.db_path+ID)\n","        success,image = vidcap.read()\n","        \n","        count = 0;\n","        while success:\n","          images.append(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n","          count += 1\n","          success,image = vidcap.read()\n","          \n","\n","        \n","\n","        \n","        X = np.empty((self.num_frames,p,*self.dim, self.n_channels))\n","            \n","        for k in range(self.num_frames):\n","            image=images[int(len(images)/self.num_frames*k)]              \n","            windows = sw.generate(image, sw.DimOrder.HeightWidthChannel,n,self.ov)\n","                \n","            for l,window in enumerate(windows):    \n","                    subset = image[window.indices()]\n","\n","                    if self.base=='vgg16':\n","                      subset=keras.applications.vgg16.preprocess_input(subset)\n","                    elif self.base=='inceptionv3':\n","                      subset=keras.applications.inception_v3.preprocess_input(subset)\n","                    elif self.base=='resnet':\n","                      subset=keras.applications.resnet.preprocess_input(subset)\n","                    elif self.base=='densenet121':\n","                      subset=keras.applications.densenet.preprocess_input(subset)\n","                    else:\n","                      print(\"No preprocessing..\")\n","                    X[k,l,:,:,:] =np.array(subset) \n","\n","                             \n","      return X"],"metadata":{"id":"SGKi7pCeuvHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Features extraction:"],"metadata":{"id":"sW-LEr2anVDJ"}},{"cell_type":"code","source":["from tensorflow.keras.layers import MaxPooling2D,Input,GlobalMaxPooling2D,GlobalAveragePooling2D,AveragePooling2D\n","from tensorflow.keras import layers"],"metadata":{"id":"hurLwcqgmIiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model =  Base_Model('resnet',weights='imagenet', include_top=False, input_shape=(224,224, 3))\n","\n","x=base_model.layers[-1].output\n","x=GlobalAveragePooling2D()(x)\n","model=keras.Model(inputs=base_model.layers[0].output,outputs=x)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CxdyZ9ml-IE","executionInfo":{"status":"ok","timestamp":1650501494536,"user_tz":-120,"elapsed":1299,"user":{"displayName":"Hanene Fatima Zohra BRACHEMI MEFTAH","userId":"05471859942716114157"}},"outputId":"b236316a-3ac8-4911-c831-76ffc138c130"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","58900480/58889256 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["import os"],"metadata":{"id":"RyL1PYIfsaTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for id in ids:\n","  generator = DataGenerator(ids=[id],**params)\n","  input=Input(shape=(p,224,224,3))\n","  output= layers.TimeDistributed(model)(input)\n","  model_cnn=keras.Model(inputs=input,outputs=output)   \n","  \n","  feature= model_cnn.predict_generator(generator=generator)\n","  print(feature.shape)\n","  np.save(os.path.join(out,id+'.npy'),feature)"],"metadata":{"id":"KaRrrJncnxrh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"d8cN5VnykrWm"},"execution_count":null,"outputs":[]}]}